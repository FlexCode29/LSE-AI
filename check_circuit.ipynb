{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a76d69a1-ce05-470b-b9ea-84dd9824d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch as t\n",
    "from easy_transformer.EasyTransformer import (\n",
    "    EasyTransformer,\n",
    ")\n",
    "from time import ctime\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from easy_transformer.experiments import (\n",
    "    ExperimentMetric,\n",
    "    AblationConfig,\n",
    "    EasyAblation,\n",
    "    EasyPatching,\n",
    "    PatchingConfig,\n",
    ")\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import einops\n",
    "from IPython import get_ipython\n",
    "from copy import deepcopy\n",
    "from easy_transformer.ioi_dataset import (\n",
    "    IOIDataset,\n",
    ")\n",
    "from easy_transformer.ioi_utils import (\n",
    "    path_patching,\n",
    "    max_2d,\n",
    "    CLASS_COLORS,\n",
    "    show_pp,\n",
    "    show_attention_patterns,\n",
    "    scatter_attention_and_contribution,\n",
    ")\n",
    "from random import randint as ri\n",
    "from easy_transformer.ioi_circuit_extraction import (\n",
    "    do_circuit_extraction,\n",
    "    get_heads_circuit,\n",
    "    CIRCUIT,\n",
    ")\n",
    "from easy_transformer.ioi_utils import logit_diff, probs\n",
    "from easy_transformer.ioi_utils import get_top_tokens_and_probs as g\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3f86e70d-7b98-4720-ab46-0d69360d5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def __init__(\n",
    "        self,\n",
    "        prompt_type: Union[\n",
    "            str, List[str]\n",
    "        ],  # if list, then it will be a list of templates\n",
    "        N=500,\n",
    "        tokenizer=None,\n",
    "        prompts=None,\n",
    "        symmetric=False,\n",
    "        prefixes=None,\n",
    "        nb_templates=None,\n",
    "        ioi_prompts_for_word_idxs=None,\n",
    "        prepend_bos=False,\n",
    "        manual_word_idx=None,\n",
    "    ):\n",
    "'''\n",
    "\n",
    "class icl_dataset:\n",
    "    def __init__(self, input, labels, N, max_len):\n",
    "        self.input = input\n",
    "        self.labels = labels\n",
    "        self.N = N\n",
    "        self.max_len = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "625b3a5b-b9b8-43c9-8d77-90eac50eab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2 into EasyTransformer!\n"
     ]
    }
   ],
   "source": [
    "model = EasyTransformer.from_pretrained(\"gpt2\")\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3882bde8-10f9-4f93-8615-25497560aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(model, device, x_initial, y_initial, icl_length, n, offset=0):\n",
    "    prompts = []\n",
    "    correct_answers = []\n",
    "    # Initialize x and y for the sequence\n",
    "    x, y = x_initial + offset, y_initial + offset\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        prompt = ''\n",
    "        for j in range(icl_length):\n",
    "            if j < icl_length - 1:\n",
    "                prompt += f\"Input: {j}, Output: {j * x + y}\\n\"\n",
    "            else:\n",
    "                prompt += f\"Input: {j}, Output:\"\n",
    "                correct_answers.append(j * x + y)  # Record the correct answer for the last input\n",
    "        prompts.append(prompt)\n",
    "        # Update x and y after generating each full prompt set\n",
    "        if i % 2 == 0:\n",
    "            x += 1\n",
    "        else:\n",
    "            y += 1\n",
    "\n",
    "\n",
    "    # Convert prompts into tokens\n",
    "    data_tokens = [model.to_tokens(prompt).to(device) for prompt in prompts]\n",
    "    correct_answers_tensor = torch.tensor(correct_answers).to(torch.double).unsqueeze(-1).to(device)\n",
    "    return icl_dataset(input=prompts, labels=correct_answers_tensor, N=n, max_len = icl_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "df6de23d-52e1-4803-a984-0efd8edae43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_metric(model, dataset, device='cpu', return_one_element = False):\n",
    "        # dataset: {input: data, labels: correct, }\n",
    "        logits = model(dataset.input, return_type=\"logits\")\n",
    "    \n",
    "    \n",
    "        # Select the logits for the last token in each sequence\n",
    "        # model_output shape: [batch_size, seq_length, vocab_size] => [10, 103, 50257]\n",
    "        # We select [:, -1, :] to get the last token logits for each example in the batch\n",
    "        last_token_logits = logits[:, -1, :]  # Shape: [10, 50257]\n",
    "    \n",
    "        # Now, find the indices of the 10 highest logits for the last token across the batch\n",
    "        # We use torch.topk to get the top 10 logits' indices for each example\n",
    "        print(last_token_logits)\n",
    "        topk_values, topk_indices = torch.topk(last_token_logits, 1, dim=1) \n",
    "\n",
    "        predictions = model.to_str_tokens(topk_indices)\n",
    "        predictions = torch.tensor([int(pred) for pred in predictions]).to(torch.double).unsqueeze(-1).to(device)\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = F.mse_loss(predictions, dataset.labels, reduction='mean' if not return_one_element else 'sum')\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "feb1ea28-4ec8-41bb-98c7-8af7dd74e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.6875,  8.7793,  9.4490,  ..., -8.7286, -5.7079, 12.5203],\n",
      "        [10.5253,  9.3165,  9.5234,  ..., -6.1724, -6.3744, 12.8294],\n",
      "        [11.2101, 10.5903, 10.3886,  ..., -6.0296, -6.8316, 12.9645],\n",
      "        ...,\n",
      "        [11.7083, 10.2900,  8.3357,  ..., -8.1260, -7.8939, 12.3668],\n",
      "        [10.9615,  9.7409,  8.2273,  ..., -7.4946, -6.8758, 11.4743],\n",
      "        [ 9.6147,  8.9172,  7.6556,  ..., -6.0987, -7.9994, 11.7625]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.2000, dtype=torch.float64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_validation_metric(device, model, x_initial, y_initial, icl_length, n):\n",
    "        dataset = generate_data(model, device, x_initial, y_initial, icl_length, n)\n",
    "        mse = validation_metric(model, dataset)\n",
    "        \n",
    "        return mse\n",
    "        print('This is the MSE: ', mse)\n",
    "test_validation_metric('cpu', model, 2, 1, 12, 10)  # 5.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f96e2ee-f3ef-4196-8734-af3b8e5455b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name mover': [(9, 9),\n",
       "  (10, 0),\n",
       "  (9, 6),\n",
       "  (10, 10),\n",
       "  (10, 6),\n",
       "  (10, 2),\n",
       "  (10, 1),\n",
       "  (11, 2),\n",
       "  (9, 7),\n",
       "  (9, 0),\n",
       "  (11, 9)],\n",
       " 'negative': [(10, 7), (11, 10)],\n",
       " 's2 inhibition': [(7, 3), (7, 9), (8, 6), (8, 10)],\n",
       " 'induction': [(5, 5), (5, 8), (5, 9), (6, 9)],\n",
       " 'duplicate token': [(0, 1), (0, 10), (3, 0)],\n",
       " 'previous token': [(2, 2), (4, 11)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIRCUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72d78fe8-6e59-4010-9312-e7b3e7ede9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICL_CIRCUIT = {\n",
    "  'operating': [(8, 1), (6, 9), (5, 0), (8, 9), (9, 11), (9, 2), (7, 7), (5, 2), (4, 8), (5, 1), (3, 4), (4, 3), (9, 9), (8, 11), (6, 10), (8, 8), (6, 0), (9, 5), (6, 3), (8, 6), (6, 7), (6, 6), (7, 6), (6, 2), (7, 10), (9, 1), (1, 9), (10, 2), (5, 11), (8, 7), (0, 1), (0, 3), (0, 5), (7, 11), (7, 2), (6, 1), (0, 8), (0, 7), (0, 6), (0, 4), (5, 5)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74053c60-fa13-4e92-bc69-c0cd4e1af319",
   "metadata": {},
   "source": [
    "# Utils to move tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9227d6d-533a-4b98-b730-bf2a74b8b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_diff(l1, l2):\n",
    "    l2_ = [int(x) for x in l2]\n",
    "    return list(set(l1).difference(set(l2_)))\n",
    "def turn_keep_into_rmv(to_keep, max_len):\n",
    "    to_rmv = {}\n",
    "    for t in to_keep.keys():\n",
    "        to_rmv[t] = []\n",
    "        for idxs in to_keep[t]:\n",
    "            to_rmv[t].append(list_diff(list(range(max_len)), idxs))\n",
    "    return to_rmv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba067d-2799-4a1a-9784-9c2ea839c39f",
   "metadata": {},
   "source": [
    "# Just get a list of Heads and MLPs to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a1370f0f-b94c-4cdf-9b52-c861d24e0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_heads_and_mlps(\n",
    "    heads_to_remove=None,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "    mlps_to_remove=None,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "    heads_to_keep=None,  # as above for heads\n",
    "    mlps_to_keep=None,  # as above for mlps\n",
    "    icl_dataset=None,\n",
    "    model=None,\n",
    "):\n",
    "    assert (heads_to_remove is None) != (heads_to_keep is None)\n",
    "    assert (mlps_to_keep is None) != (mlps_to_remove is None)\n",
    "\n",
    "    n_layers = model.cfg.n_layers\n",
    "    n_heads = model.cfg.n_heads\n",
    "\n",
    "    dataset_length = icl_dataset.N\n",
    "\n",
    "    #commented out since I only want to remove attention\n",
    "    if mlps_to_remove is not None:\n",
    "        mlps = mlps_to_remove.copy()\n",
    "    else:  # MARCO, if list of mlps to remove available just use, otherwise remove all not in 'to keep'. it do smart computation in mean cache\n",
    "        mlps = mlps_to_keep.copy()\n",
    "        for l in range(n_layers):\n",
    "            if l not in mlps_to_keep:\n",
    "                mlps[l] = [[] for _ in range(dataset_length)]\n",
    "        mlps = turn_keep_into_rmv(\n",
    "            mlps, icl_dataset.max_len\n",
    "        )  # TODO check that this is still right for the max_len of maybe shortened datasets\n",
    "\n",
    "    # MARCO Same as MLP above\n",
    "    if heads_to_remove is not None:\n",
    "        heads = heads_to_remove.copy()\n",
    "    else:\n",
    "        heads = heads_to_keep.copy()\n",
    "        for l in range(n_layers):\n",
    "            for h in range(n_heads):\n",
    "                if (l, h) not in heads_to_keep:\n",
    "                    heads[(l, h)] = [[] for _ in range(dataset_length)]\n",
    "        heads = turn_keep_into_rmv(heads, icl_dataset.max_len)\n",
    "    return heads, mlps\n",
    "    # print(mlps, heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18f39e-f755-4005-b6a4-caed2f80c2a4",
   "metadata": {},
   "source": [
    "# Returns the hooks for z mlp and heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "773656c3-4687-4ec3-a80c-cd606d1315af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_circuit_replacement_hook(\n",
    "    heads_to_remove=None,\n",
    "    mlps_to_remove=None,\n",
    "    heads_to_keep=None,\n",
    "    mlps_to_keep=None,\n",
    "    heads_to_remove2=None,  # TODO @Alex ehat are these\n",
    "    mlps_to_remove2=None,\n",
    "    heads_to_keep2=None,\n",
    "    mlps_to_keep2=None,\n",
    "    icl_dataset=None,\n",
    "    model=None,\n",
    "):\n",
    "    # MARCO function above, just get a list\n",
    "    heads, mlps = process_heads_and_mlps(\n",
    "        heads_to_remove=heads_to_remove,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "        mlps_to_remove=mlps_to_remove,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "        heads_to_keep=heads_to_keep,  # as above for heads\n",
    "        mlps_to_keep=mlps_to_keep,  # as above for mlps\n",
    "        icl_dataset=icl_dataset,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    if (heads_to_remove2 is not None) or (heads_to_keep2 is not None):\n",
    "        heads2, mlps2 = process_heads_and_mlps(\n",
    "            heads_to_remove=heads_to_remove2,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "            mlps_to_remove=mlps_to_remove2,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "            heads_to_keep=heads_to_keep2,  # as above for heads\n",
    "            mlps_to_keep=mlps_to_keep2,  # as above for mlps\n",
    "            icl_dataset=icl_dataset,\n",
    "            model=model,\n",
    "        )\n",
    "    else:\n",
    "        heads2, mlps2 = heads, mlps\n",
    "\n",
    "    dataset_length = icl_dataset.N\n",
    "\n",
    "    def circuit_replmt_hook(z, act, hook):  # batch, seq, heads, head dim\n",
    "        layer = int(hook.name.split(\".\")[1])\n",
    "        if \"mlp\" in hook.name and layer in mlps:\n",
    "            for i in range(dataset_length):\n",
    "                z[i, mlps[layer][i], :] = act[\n",
    "                    i, mlps2[layer][i], :\n",
    "                ]  # ablate all the indices in mlps[layer][i]; mean may contain semantic ablation\n",
    "                # TODO can this i loop be vectorized?\n",
    "\n",
    "        if \"attn.hook_result\" in hook.name and (layer, hook.ctx[\"idx\"]) in heads:\n",
    "            for i in range(\n",
    "                dataset_length\n",
    "            ):  # we use the idx from contex to get the head\n",
    "                z[i, heads[(layer, hook.ctx[\"idx\"])][i], :] = act[\n",
    "                    i,\n",
    "                    heads2[(layer, hook.ctx[\"idx\"])][i],\n",
    "                    :,\n",
    "                ]\n",
    "\n",
    "        return z\n",
    "\n",
    "    return circuit_replmt_hook, heads, mlps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "246f7c1e-4904-46eb-adc6-e442ccf97abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_circuit_extraction(\n",
    "    heads_to_remove=None,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "    mlps_to_remove=None,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "    heads_to_keep=None,  # as above for heads\n",
    "    mlps_to_keep=None,  # as above for mlps\n",
    "    ioi_dataset=None,\n",
    "    mean_dataset=None,\n",
    "    model=None,\n",
    "    metric=None,\n",
    "    excluded=[],  # tuple of (layer, head) or (layer, None for MLPs)\n",
    "    return_hooks=False,\n",
    "    hooks_dict=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    ..._to_remove means the indices ablated away. Otherwise the indices not ablated away.\n",
    "\n",
    "    `exclude_heads` is a list of heads that actually we won't put any hooks on. Just keep them as is\n",
    "\n",
    "    if `mean_dataset` is None, just use the ioi_dataset for mean\n",
    "    \"\"\"\n",
    "\n",
    "    # check if we are either in keep XOR remove move from the args\n",
    "    ablation, heads, mlps = get_circuit_replacement_hook(\n",
    "        heads_to_remove=heads_to_remove,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "        mlps_to_remove=mlps_to_remove,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "        heads_to_keep=heads_to_keep,  # as above for heads\n",
    "        mlps_to_keep=mlps_to_keep,  # as above for mlps\n",
    "        ioi_dataset=ioi_dataset,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    metric = ExperimentMetric(\n",
    "        metric=metric, dataset=ioi_dataset.sentences, relative_metric=False\n",
    "    )  # TODO make dummy metric\n",
    "\n",
    "    if mean_dataset is None:\n",
    "        mean_dataset = ioi_dataset\n",
    "\n",
    "    config = AblationConfig(\n",
    "        abl_type=\"custom\",\n",
    "        abl_fn=ablation,\n",
    "        mean_dataset=mean_dataset.toks.long(),  # TODO nb of prompts useless ?\n",
    "        target_module=\"attn_head\",\n",
    "        head_circuit=\"result\",\n",
    "        cache_means=True,  # circuit extraction *has* to cache means. the get_mean reset the\n",
    "        verbose=True,\n",
    "    )\n",
    "    abl = EasyAblation(\n",
    "        model,\n",
    "        config,\n",
    "        metric,\n",
    "        semantic_indices=None,  # ioi_dataset.sem_tok_idx,\n",
    "        mean_by_groups=True,  # TO CHECK CIRCUIT BY GROUPS\n",
    "        groups=ioi_dataset.groups,\n",
    "    )\n",
    "    model.reset_hooks()\n",
    "\n",
    "    hooks = {}\n",
    "\n",
    "    heads_keys = list(heads.keys())\n",
    "    # sort in lexicographic order\n",
    "    heads_keys.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    for (\n",
    "        layer,\n",
    "        head,\n",
    "    ) in heads_keys:  # a sketchy edit here didn't really improve things : (\n",
    "        if (layer, head) in excluded:\n",
    "            continue\n",
    "        assert (layer, head) not in hooks, ((layer, head), \"already in hooks\")\n",
    "        hooks[(layer, head)] = abl.get_hook(layer, head)\n",
    "        # model.add_hook(*abl.get_hook(layer, head))\n",
    "    for layer in mlps.keys():\n",
    "        hooks[(layer, None)] = abl.get_hook(layer, head=None, target_module=\"mlp\")\n",
    "        # model.add_hook(*abl.get_hook(layer, head=None, target_module=\"mlp\"))\n",
    "\n",
    "    if return_hooks:\n",
    "        if hooks_dict:\n",
    "            return hooks\n",
    "        else:\n",
    "            return list(hooks.values())\n",
    "\n",
    "    else:\n",
    "        for hook in hooks.values():\n",
    "            model.add_hook(*hook)\n",
    "        return model, abl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60ddc2a5-69dd-4b58-ba97-c6fa25cf0c7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_extracted_idx\u001b[39m(idx_list: \u001b[43mList\u001b[49m[\u001b[38;5;28mstr\u001b[39m], ioi_dataset):\n\u001b[1;32m      2\u001b[0m     int_idx \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ioi_dataset\u001b[38;5;241m.\u001b[39msentences))]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx_name \u001b[38;5;129;01min\u001b[39;00m idx_list:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def get_extracted_idx(idx_list: List[str], ioi_dataset):\n",
    "    int_idx = [[] for i in range(len(ioi_dataset.sentences))]\n",
    "    for idx_name in idx_list:\n",
    "        try:\n",
    "            int_idx_to_add = [\n",
    "                int(x) for x in list(ioi_dataset.word_idx[idx_name])\n",
    "            ]  # torch to python objects\n",
    "        except:\n",
    "            print(ioi_dataset.word_idx, idx_name)\n",
    "            raise ValueError(\n",
    "                f\"Index {idx_name} not found in the dataset. Please check the spelling and make sure the index is in the dataset.\"\n",
    "            )\n",
    "        int_idx = join_lists(int_idx, int_idx_to_add)\n",
    "    return int_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4aeb1936-e182-4bef-a47b-42d650acb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heads_circuit(ioi_dataset, excluded=[], mlp0=False, circuit=CIRCUIT):\n",
    "    for excluded_thing in excluded:\n",
    "        assert (\n",
    "            isinstance(excluded_thing, tuple) or excluded_thing in circuit.keys()\n",
    "        ), excluded_thing\n",
    "\n",
    "    heads_to_keep = {}\n",
    "\n",
    "    for circuit_class in circuit.keys():\n",
    "        if circuit_class in excluded:\n",
    "            continue\n",
    "        for head in circuit[circuit_class]:\n",
    "            if head in excluded:\n",
    "                continue\n",
    "            heads_to_keep[head] = get_extracted_idx(RELEVANT_TOKENS[head], ioi_dataset)\n",
    "\n",
    "    if mlp0:\n",
    "        raise ValueError(\"Arthur moved this to get_mlps_circuit\")\n",
    "\n",
    "    return heads_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "819ca83d-f2ce-4b81-8c76-b0092681b12e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_extracted_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# we then add hooks to the model to knockout all the heads except the circuit\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hooks()\n\u001b[1;32m      9\u001b[0m model, _ \u001b[38;5;241m=\u001b[39m do_circuit_extraction(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m---> 11\u001b[0m     heads_to_keep\u001b[38;5;241m=\u001b[39m\u001b[43mget_heads_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mioi_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     12\u001b[0m     mlps_to_remove\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m     13\u001b[0m     ioi_dataset\u001b[38;5;241m=\u001b[39mbase_dataset,\n\u001b[1;32m     14\u001b[0m     mean_dataset\u001b[38;5;241m=\u001b[39mpatch_dataset,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m circuit_mse_diff \u001b[38;5;241m=\u001b[39m validation_metric(model, base_dataset)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe circuit gets average logit difference \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcircuit_mse_diff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n",
      "Cell \u001b[0;32mIn[77], line 15\u001b[0m, in \u001b[0;36mget_heads_circuit\u001b[0;34m(ioi_dataset, excluded, mlp0, circuit)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m excluded:\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m         heads_to_keep[head] \u001b[38;5;241m=\u001b[39m \u001b[43mget_extracted_idx\u001b[49m(RELEVANT_TOKENS[head], ioi_dataset)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mlp0:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArthur moved this to get_mlps_circuit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_extracted_idx' is not defined"
     ]
    }
   ],
   "source": [
    "circuit = deepcopy(ICL_CIRCUIT)\n",
    "\n",
    "# we make the ABC dataset in order to knockout other model components\n",
    "# generate_data(model, device, x_initial, y_initial, icl_length, n, offset=0)\n",
    "base_dataset = generate_data(model, 'cpu', 2, 1, 12, 10, 0)\n",
    "patch_dataset = generate_data(model, 'cpu', 2, 1, 12, 10, 20)\n",
    "# we then add hooks to the model to knockout all the heads except the circuit\n",
    "model.reset_hooks()\n",
    "model, _ = do_circuit_extraction(\n",
    "    model=model,\n",
    "    heads_to_keep=get_heads_circuit(ioi_dataset=base_dataset, circuit=circuit),\n",
    "    mlps_to_remove={},\n",
    "    ioi_dataset=base_dataset,\n",
    "    mean_dataset=patch_dataset,\n",
    ")\n",
    "\n",
    "circuit_mse_diff = validation_metric(model, base_dataset)\n",
    "print(\n",
    "    f\"The circuit gets average logit difference {circuit_mse_diff} over {N} examples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9b54e-541b-4303-9284-5f0f3dcc0c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
