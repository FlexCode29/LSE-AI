{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76d69a1-ce05-470b-b9ea-84dd9824d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142/1394111376.py:54: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_142/1394111376.py:55: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch as t\n",
    "from easy_transformer.EasyTransformer import (\n",
    "    EasyTransformer,\n",
    ")\n",
    "from time import ctime\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from easy_transformer.experiments import (\n",
    "    ExperimentMetric,\n",
    "    AblationConfig,\n",
    "    EasyAblation,\n",
    "    EasyPatching,\n",
    "    PatchingConfig,\n",
    ")\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import einops\n",
    "from IPython import get_ipython\n",
    "from copy import deepcopy\n",
    "from easy_transformer.ioi_dataset import (\n",
    "    IOIDataset,\n",
    ")\n",
    "from easy_transformer.ioi_utils import (\n",
    "    path_patching,\n",
    "    max_2d,\n",
    "    CLASS_COLORS,\n",
    "    show_pp,\n",
    "    show_attention_patterns,\n",
    "    scatter_attention_and_contribution,\n",
    ")\n",
    "from random import randint as ri\n",
    "from easy_transformer.ioi_circuit_extraction import (\n",
    "    do_circuit_extraction,\n",
    "    get_heads_circuit,\n",
    "    CIRCUIT,\n",
    ")\n",
    "from easy_transformer.ioi_utils import logit_diff, probs\n",
    "from easy_transformer.ioi_utils import get_top_tokens_and_probs as g\n",
    "\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625b3a5b-b9b8-43c9-8d77-90eac50eab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/easy_transformer/components.py:616: UserWarning: Moved LN1 to the attention block\n",
      "  warnings.warn(\"Moved LN1 to the attention block\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2 into EasyTransformer!\n"
     ]
    }
   ],
   "source": [
    "model = EasyTransformer.from_pretrained(\"gpt2\")\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d12efe-4e80-4008-af50-e8ec52c2e64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name mover': [(9, 9),\n",
       "  (10, 0),\n",
       "  (9, 6),\n",
       "  (10, 10),\n",
       "  (10, 6),\n",
       "  (10, 2),\n",
       "  (10, 1),\n",
       "  (11, 2),\n",
       "  (9, 7),\n",
       "  (9, 0),\n",
       "  (11, 9)],\n",
       " 'negative': [(10, 7), (11, 10)],\n",
       " 's2 inhibition': [(7, 3), (7, 9), (8, 6), (8, 10)],\n",
       " 'induction': [(5, 5), (5, 8), (5, 9), (6, 9)],\n",
       " 'duplicate token': [(0, 1), (0, 10), (3, 0)],\n",
       " 'previous token': [(2, 2), (4, 11)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIRCUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3882bde8-10f9-4f93-8615-25497560aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(model, device, x_initial, y_initial, icl_length, n, offset=0):\n",
    "    prompts = []\n",
    "    correct_answers = []\n",
    "    # Initialize x and y for the sequence\n",
    "    x, y = x_initial + offset, y_initial + offset\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        prompt = ''\n",
    "        for j in range(icl_length + 1):\n",
    "            if j < icl_length:\n",
    "                prompt += f\"Input: {j}, Output: {j * x + y}\\n\"\n",
    "            else:\n",
    "                prompt += f\"Input: {j}, Output:\"\n",
    "                correct_answers.append(j * x + y)  # Record the correct answer for the last input\n",
    "        prompts.append(prompt)\n",
    "        # Update x and y after generating each full prompt set\n",
    "        if i % 2 == 0:\n",
    "            x += 1\n",
    "        else:\n",
    "            y += 1\n",
    "\n",
    "\n",
    "    # Convert prompts into tokens\n",
    "    data_tokens = [model.to_tokens(prompt).to(device) for prompt in prompts]\n",
    "    correct_answers_tensor = torch.tensor(correct_answers).to(torch.double).unsqueeze(-1).to(device)\n",
    "    return prompts, correct_answers_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df6de23d-52e1-4803-a984-0efd8edae43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_metric(output, model, correct, return_one_element, device):\n",
    "\n",
    "        output = output.to(device)\n",
    "    \n",
    "        # Select the logits for the last token in each sequence\n",
    "        # model_output shape: [batch_size, seq_length, vocab_size] => [10, 103, 50257]\n",
    "        # We select [:, -1, :] to get the last token logits for each example in the batch\n",
    "        last_token_logits = output[:, -1, :]  # Shape: [10, 50257]\n",
    "    \n",
    "        # Now, find the indices of the 10 highest logits for the last token across the batch\n",
    "        # We use torch.topk to get the top 10 logits' indices for each example\n",
    "        topk_values, topk_indices = torch.topk(last_token_logits, 1, dim=1) \n",
    "\n",
    "        predictions = model.to_str_tokens(topk_indices)\n",
    "        predictions = torch.tensor([int(pred) for pred in predictions]).to(torch.double).unsqueeze(-1).to(device)\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = F.mse_loss(predictions, correct, reduction='mean' if not return_one_element else 'sum')\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "feb1ea28-4ec8-41bb-98c7-8af7dd74e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the MSE:  tensor(5.7000, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def test_validation_metric(device, model, x_initial, y_initial, icl_length, n):\n",
    "        data, correct_answers = generate_data(model, device, x_initial, y_initial, icl_length, n)\n",
    "        # Assuming validation_metric is defined elsewhere in your code\n",
    "        logits = model(data, return_type=\"logits\")\n",
    "        mse = validation_metric(model=model, output=logits, correct=correct_answers, return_one_element=False, device=device)\n",
    "\n",
    "        print('This is the MSE: ', mse)\n",
    "test_validation_metric('cpu', model, 2, 1, 12, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96e2ee-f3ef-4196-8734-af3b8e5455b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
