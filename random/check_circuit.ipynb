{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a76d69a1-ce05-470b-b9ea-84dd9824d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch as t\n",
    "from easy_transformer.EasyTransformer import (\n",
    "    EasyTransformer,\n",
    ")\n",
    "from time import ctime\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from easy_transformer.experiments import (\n",
    "    ExperimentMetric,\n",
    "    AblationConfig,\n",
    "    EasyAblation,\n",
    "    EasyPatching,\n",
    "    PatchingConfig,\n",
    ")\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import einops\n",
    "from IPython import get_ipython\n",
    "from copy import deepcopy\n",
    "from easy_transformer.ioi_dataset import (\n",
    "    IOIDataset,\n",
    ")\n",
    "from easy_transformer.ioi_utils import (\n",
    "    path_patching,\n",
    "    max_2d,\n",
    "    CLASS_COLORS,\n",
    "    show_pp,\n",
    "    show_attention_patterns,\n",
    "    scatter_attention_and_contribution,\n",
    ")\n",
    "from random import randint as ri\n",
    "from easy_transformer.ioi_circuit_extraction import (\n",
    "    do_circuit_extraction,\n",
    "    get_heads_circuit,\n",
    "    CIRCUIT,\n",
    ")\n",
    "from easy_transformer.ioi_utils import logit_diff, probs\n",
    "from easy_transformer.ioi_utils import get_top_tokens_and_probs as g\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3f86e70d-7b98-4720-ab46-0d69360d5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def __init__(\n",
    "        self,\n",
    "        prompt_type: Union[\n",
    "            str, List[str]\n",
    "        ],  # if list, then it will be a list of templates\n",
    "        N=500,\n",
    "        tokenizer=None,\n",
    "        prompts=None,\n",
    "        symmetric=False,\n",
    "        prefixes=None,\n",
    "        nb_templates=None,\n",
    "        ioi_prompts_for_word_idxs=None,\n",
    "        prepend_bos=False,\n",
    "        manual_word_idx=None,\n",
    "    ):\n",
    "'''\n",
    "\n",
    "class icl_dataset:\n",
    "    def __init__(self, input, labels, N, max_len, device):\n",
    "        self.input = input.to(device)\n",
    "        self.labels = labels\n",
    "        self.N = N\n",
    "        self.max_len = max_len\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "625b3a5b-b9b8-43c9-8d77-90eac50eab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2 into EasyTransformer!\n"
     ]
    }
   ],
   "source": [
    "model = EasyTransformer.from_pretrained(\"gpt2\")\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3882bde8-10f9-4f93-8615-25497560aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(model, device, x_initial, y_initial, icl_length, n, offset=0):\n",
    "    prompts = []\n",
    "    correct_answers = []\n",
    "    # Initialize x and y for the sequence\n",
    "    x, y = x_initial + offset, y_initial + offset\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        prompt = ''\n",
    "        for j in range(icl_length):\n",
    "            if j < icl_length - 1:\n",
    "                prompt += f\"Input: {j}, Output: {j * x + y}\\n\"\n",
    "            else:\n",
    "                prompt += f\"Input: {j}, Output:\"\n",
    "                correct_answers.append(j * x + y)  # Record the correct answer for the last input\n",
    "        prompts.append(prompt)\n",
    "        # Update x and y after generating each full prompt set\n",
    "        if i % 2 == 0:\n",
    "            x += 1\n",
    "        else:\n",
    "            y += 1\n",
    "\n",
    "\n",
    "    # Convert prompts into tokens\n",
    "    data_tokens = model.to_tokens(prompts).to(device)\n",
    "    correct_answers_tensor = torch.tensor(correct_answers).to(torch.double).unsqueeze(-1).to(device)\n",
    "    return icl_dataset(input=data_tokens, labels=correct_answers_tensor, N=n, max_len = icl_length, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "df6de23d-52e1-4803-a984-0efd8edae43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_metric(model, dataset, device='cpu', return_one_element = False):\n",
    "        # dataset: {input: data, labels: correct, }\n",
    "        logits = model(dataset.input, return_type=\"logits\")\n",
    "    \n",
    "    \n",
    "        # Select the logits for the last token in each sequence\n",
    "        # model_output shape: [batch_size, seq_length, vocab_size] => [10, 103, 50257]\n",
    "        # We select [:, -1, :] to get the last token logits for each example in the batch\n",
    "        last_token_logits = logits[:, -1, :]  # Shape: [10, 50257]\n",
    "    \n",
    "        # Now, find the indices of the 10 highest logits for the last token across the batch\n",
    "        # We use torch.topk to get the top 10 logits' indices for each example\n",
    "        topk_values, topk_indices = torch.topk(last_token_logits, 1, dim=1) \n",
    "\n",
    "        predictions = model.to_str_tokens(topk_indices)\n",
    "        predictions = torch.tensor([int(pred) for pred in predictions]).to(torch.double).unsqueeze(-1).to(device)\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse = F.mse_loss(predictions, dataset.labels, reduction='mean' if not return_one_element else 'sum')\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "feb1ea28-4ec8-41bb-98c7-8af7dd74e1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2000, dtype=torch.float64)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_validation_metric(device, model, x_initial, y_initial, icl_length, n):\n",
    "        dataset = generate_data(model, device, x_initial, y_initial, icl_length, n)\n",
    "        mse = validation_metric(model, dataset)\n",
    "        \n",
    "        return mse\n",
    "        print('This is the MSE: ', mse)\n",
    "test_validation_metric('cpu', model, 2, 1, 12, 10)  # 5.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "72d78fe8-6e59-4010-9312-e7b3e7ede9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICL_CIRCUIT = {\n",
    "  'operating': [(8, 1), (6, 9), (5, 0), (8, 9), (9, 11), (9, 2), (7, 7), (5, 2), (4, 8), (5, 1), (3, 4), (4, 3), (9, 9), (8, 11), (6, 10), (8, 8), (6, 0), (9, 5), (6, 3), (8, 6), (6, 7), (6, 6), (7, 6), (6, 2), (7, 10), (9, 1), (1, 9), (10, 2), (5, 11), (8, 7), (0, 1), (0, 3), (0, 5), (7, 11), (7, 2), (6, 1), (0, 8), (0, 7), (0, 6), (0, 4), (5, 5)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74053c60-fa13-4e92-bc69-c0cd4e1af319",
   "metadata": {},
   "source": [
    "# Utils to move tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d9227d6d-533a-4b98-b730-bf2a74b8b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_diff(l1, l2):\n",
    "    l2_ = [int(x) for x in l2]\n",
    "    return list(set(l1).difference(set(l2_)))\n",
    "def turn_keep_into_rmv(to_keep, max_len):\n",
    "    print(to_keep, max_len\n",
    "    to_rmv = {}\n",
    "    for t in to_keep.keys():\n",
    "        to_rmv[t] = []\n",
    "        for idxs in to_keep[t]:\n",
    "            to_rmv[t].append(list_diff(list(range(max_len)), idxs))\n",
    "    return to_rmv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba067d-2799-4a1a-9784-9c2ea839c39f",
   "metadata": {},
   "source": [
    "# Just get a list of Heads and MLPs to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a1370f0f-b94c-4cdf-9b52-c861d24e0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_heads_and_mlps(\n",
    "    heads_to_remove=None,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "    mlps_to_remove=None,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "    heads_to_keep=None,  # as above for heads\n",
    "    mlps_to_keep=None,  # as above for mlps\n",
    "    ioi_dataset=None,\n",
    "    model=None,\n",
    "):\n",
    "    assert (heads_to_remove is None) != (heads_to_keep is None)\n",
    "    assert (mlps_to_keep is None) != (mlps_to_remove is None)\n",
    "\n",
    "    n_layers = model.cfg.n_layers\n",
    "    n_heads = model.cfg.n_heads\n",
    "\n",
    "    dataset_length = ioi_dataset.N\n",
    "\n",
    "    #commented out since I only want to remove attention\n",
    "    if mlps_to_remove is not None:\n",
    "        mlps = mlps_to_remove.copy()\n",
    "    else:  # MARCO, if list of mlps to remove available just use, otherwise remove all not in 'to keep'. it do smart computation in mean cache\n",
    "        mlps = mlps_to_keep.copy()\n",
    "        for l in range(n_layers):\n",
    "            if l not in mlps_to_keep:\n",
    "                mlps[l] = [[] for _ in range(dataset_length)]\n",
    "        mlps = turn_keep_into_rmv(\n",
    "            mlps, ioi_dataset.max_len\n",
    "        )  # TODO check that this is still right for the max_len of maybe shortened datasets\n",
    "\n",
    "    # MARCO Same as MLP above\n",
    "    if heads_to_remove is not None:\n",
    "        heads = heads_to_remove.copy()\n",
    "    else:\n",
    "        heads = heads_to_keep.copy()\n",
    "        for l in range(n_layers):\n",
    "            for h in range(n_heads):\n",
    "                if (l, h) not in heads_to_keep:\n",
    "                    heads[(l, h)] = [[] for _ in range(dataset_length)]\n",
    "        heads = turn_keep_into_rmv(heads, ioi_dataset.max_len)\n",
    "    return heads, mlps\n",
    "    # print(mlps, heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18f39e-f755-4005-b6a4-caed2f80c2a4",
   "metadata": {},
   "source": [
    "# Returns the hooks for z mlp and heads, then ablate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "773656c3-4687-4ec3-a80c-cd606d1315af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_circuit_replacement_hook(\n",
    "    heads_to_remove=None,\n",
    "    mlps_to_remove=None,\n",
    "    heads_to_keep=None,\n",
    "    mlps_to_keep=None,\n",
    "    heads_to_remove2=None,  # TODO @Alex ehat are these\n",
    "    mlps_to_remove2=None,\n",
    "    heads_to_keep2=None,\n",
    "    mlps_to_keep2=None,\n",
    "    ioi_dataset=None,\n",
    "    model=None,\n",
    "):\n",
    "    # MARCO function above, just get a list\n",
    "    heads, mlps = process_heads_and_mlps(\n",
    "        heads_to_remove=heads_to_remove,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "        mlps_to_remove=mlps_to_remove,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "        heads_to_keep=heads_to_keep,  # as above for heads\n",
    "        mlps_to_keep=mlps_to_keep,  # as above for mlps\n",
    "        ioi_dataset=ioi_dataset,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    if (heads_to_remove2 is not None) or (heads_to_keep2 is not None):\n",
    "        heads2, mlps2 = process_heads_and_mlps(\n",
    "            heads_to_remove=heads_to_remove2,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "            mlps_to_remove=mlps_to_remove2,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "            heads_to_keep=heads_to_keep2,  # as above for heads\n",
    "            mlps_to_keep=mlps_to_keep2,  # as above for mlps\n",
    "            ioi_dataset=ioi_dataset,\n",
    "            model=model,\n",
    "        )\n",
    "    else:\n",
    "        heads2, mlps2 = heads, mlps\n",
    "\n",
    "    dataset_length = ioi_dataset.N\n",
    "\n",
    "    def circuit_replmt_hook(z, act, hook):  # batch, seq, heads, head dim\n",
    "        layer = int(hook.name.split(\".\")[1])\n",
    "        if \"mlp\" in hook.name and layer in mlps:\n",
    "            for i in range(dataset_length):\n",
    "                z[i, mlps[layer][i], :] = act[\n",
    "                    i, mlps2[layer][i], :\n",
    "                ]  # ablate all the indices in mlps[layer][i]; mean may contain semantic ablation\n",
    "                # TODO can this i loop be vectorized?\n",
    "\n",
    "        if \"attn.hook_result\" in hook.name and (layer, hook.ctx[\"idx\"]) in heads:\n",
    "            for i in range(\n",
    "                dataset_length\n",
    "            ):  # we use the idx from contex to get the head\n",
    "                z[i, heads[(layer, hook.ctx[\"idx\"])][i], :] = act[\n",
    "                    i,\n",
    "                    heads2[(layer, hook.ctx[\"idx\"])][i],\n",
    "                    :,\n",
    "                ]\n",
    "\n",
    "        return z\n",
    "\n",
    "    return circuit_replmt_hook, heads, mlps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "246f7c1e-4904-46eb-adc6-e442ccf97abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_circuit_extraction(\n",
    "    heads_to_remove=None,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "    mlps_to_remove=None,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "    heads_to_keep=None,  # as above for heads\n",
    "    mlps_to_keep=None,  # as above for mlps\n",
    "    ioi_dataset=None,\n",
    "    mean_dataset=None,\n",
    "    model=None,\n",
    "    metric=None,\n",
    "    excluded=[],  # tuple of (layer, head) or (layer, None for MLPs)\n",
    "    return_hooks=False,\n",
    "    hooks_dict=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    ..._to_remove means the indices ablated away. Otherwise the indices not ablated away.\n",
    "\n",
    "    `exclude_heads` is a list of heads that actually we won't put any hooks on. Just keep them as is\n",
    "\n",
    "    if `mean_dataset` is None, just use the ioi_dataset for mean\n",
    "    \"\"\"\n",
    "\n",
    "    # check if we are either in keep XOR remove move from the args\n",
    "    ablation, heads, mlps = get_circuit_replacement_hook(\n",
    "        heads_to_remove=heads_to_remove,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "        mlps_to_remove=mlps_to_remove,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
    "        heads_to_keep=heads_to_keep,  # as above for heads\n",
    "        mlps_to_keep=mlps_to_keep,  # as above for mlps\n",
    "        ioi_dataset=ioi_dataset,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    metric = ExperimentMetric(\n",
    "        metric=metric, dataset=ioi_dataset.input, relative_metric=False\n",
    "    )  # TODO make dummy metric\n",
    "\n",
    "    if mean_dataset is None:\n",
    "        mean_dataset = ioi_dataset\n",
    "\n",
    "    config = AblationConfig(\n",
    "        abl_type=\"custom\",\n",
    "        abl_fn=ablation,\n",
    "        mean_dataset=mean_dataset.input.long(), \n",
    "        target_module=\"attn_head\",\n",
    "        head_circuit=\"result\",\n",
    "        cache_means=True,  # circuit extraction *has* to cache means. the get_mean reset the\n",
    "        verbose=True,\n",
    "    )\n",
    "    abl = EasyAblation(\n",
    "        model,\n",
    "        config,\n",
    "        metric,\n",
    "        semantic_indices=None,  # ioi_dataset.sem_tok_idx,\n",
    "    )\n",
    "    model.reset_hooks()\n",
    "\n",
    "    hooks = {}\n",
    "\n",
    "    heads_keys = list(heads.keys())\n",
    "    # sort in lexicographic order\n",
    "    heads_keys.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    for (\n",
    "        layer,\n",
    "        head,\n",
    "    ) in heads_keys:  # a sketchy edit here didn't really improve things : (\n",
    "        if (layer, head) in excluded:\n",
    "            continue\n",
    "        assert (layer, head) not in hooks, ((layer, head), \"already in hooks\")\n",
    "        hooks[(layer, head)] = abl.get_hook(layer, head)\n",
    "        # model.add_hook(*abl.get_hook(layer, head))\n",
    "    for layer in mlps.keys():\n",
    "        hooks[(layer, None)] = abl.get_hook(layer, head=None, target_module=\"mlp\")\n",
    "        # model.add_hook(*abl.get_hook(layer, head=None, target_module=\"mlp\"))\n",
    "\n",
    "    if return_hooks:\n",
    "        if hooks_dict:\n",
    "            return hooks\n",
    "        else:\n",
    "            return list(hooks.values())\n",
    "\n",
    "    else:\n",
    "        for hook in hooks.values():\n",
    "            model.add_hook(*hook)\n",
    "        return model, abl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4aeb1936-e182-4bef-a47b-42d650acb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heads_circuit(ioi_dataset, excluded=[], mlp0=False, circuit=CIRCUIT):\n",
    "\n",
    "    for excluded_thing in excluded:\n",
    "        assert (\n",
    "            isinstance(excluded_thing, tuple) or excluded_thing in circuit.keys()\n",
    "        ), excluded_thing\n",
    "\n",
    "    heads_to_keep = {}\n",
    "    \n",
    "\n",
    "    for circuit_class in circuit.keys():\n",
    "        if circuit_class in excluded:\n",
    "            continue\n",
    "        for head in circuit[circuit_class]:\n",
    "            if head in excluded:\n",
    "                continue\n",
    "            heads_to_keep[head] = list(range(ioi_dataset.N))\n",
    "\n",
    "    if mlp0:\n",
    "        raise ValueError(\"Arthur moved this to get_mlps_circuit\")\n",
    "\n",
    "    return heads_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "819ca83d-f2ce-4b81-8c76-b0092681b12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The circuit has mse 3.2 over 10 examples. Full model mse was 3.2\n",
      "The circuit has mse 8207.5 over 10 examples. Full model mse was 8207.5\n"
     ]
    }
   ],
   "source": [
    "circuit = deepcopy(ICL_CIRCUIT)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "N = 10\n",
    "\n",
    "# we make the ABC dataset in order to knockout other model components\n",
    "# generate_data(model, device, x_initial, y_initial, icl_length, n, offset=0)\n",
    "base_dataset = generate_data(model, device, 2, 1, 12, N, 0)\n",
    "benchmark_mse = validation_metric(model, base_dataset)\n",
    "\n",
    "patch_dataset = generate_data(model, device, 2, 1, 12, N, 20)\n",
    "benchmark_patch_mse = validation_metric(model, patch_dataset)\n",
    "\n",
    "# we then add hooks to the model to knockout all the heads except the circuit\n",
    "model.reset_hooks()\n",
    "model, _ = do_circuit_extraction(\n",
    "    model=model,\n",
    "    heads_to_keep=get_heads_circuit(ioi_dataset=base_dataset, circuit=circuit),\n",
    "    mlps_to_remove={},\n",
    "    ioi_dataset=base_dataset,\n",
    "    mean_dataset=patch_dataset\n",
    ")\n",
    "\n",
    "circuit_mse = validation_metric(model, base_dataset)\n",
    "print(\n",
    "    f\"The circuit has mse {circuit_mse} over {N} examples. Full model mse was {benchmark_mse}\"\n",
    ")\n",
    "\n",
    "circuit_patch_mse = validation_metric(model, patch_dataset)\n",
    "print(\n",
    "    f\"The circuit has mse {circuit_patch_mse} over {N} examples. Full model mse was {benchmark_patch_mse}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca3a1a-f894-4da1-84c0-7813cfe8d70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
